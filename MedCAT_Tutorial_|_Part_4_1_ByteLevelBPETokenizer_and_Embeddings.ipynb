{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MedCAT Tutorial | Part 4.1 - ByteLevelBPETokenizer and Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophie-w/COVID-MDS/blob/master/MedCAT_Tutorial_%7C_Part_4_1_ByteLevelBPETokenizer_and_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXXf2emwSdpN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4ae7cec-122e-41c8-a359-73e0a6769b16"
      },
      "source": [
        "! pip install --upgrade medcat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting medcat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/b5/62d61238e8929b5ee718e7bb8fed7559702f10c32c3309ebb23d075c64bd/medcat-1.0.33-py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 9.8MB/s \n",
            "\u001b[?25hCollecting elasticsearch~=7.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/b1/58cfb0bf54e29c20669d6e588496fb7fe8b54f53bc238be4cb0a185a1e76/elasticsearch-7.13.1-py2.py3-none-any.whl (354kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 16.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pandas~=1.0 in /usr/local/lib/python3.7/dist-packages (from medcat) (1.1.5)\n",
            "Collecting numpy~=1.20\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/42/560d269f604d3e186a57c21a363e77e199358d054884e61b73e405dd217c/numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 268kB/s \n",
            "\u001b[?25hCollecting datasets~=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: sklearn~=0.0 in /usr/local/lib/python3.7/dist-packages (from medcat) (0.0)\n",
            "Collecting spacy==2.3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/92/ce37391be0980e03cdef9dab057c95561dacb36937bd6941c3204b40f5ca/spacy-2.3.4-cp37-cp37m-manylinux2014_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 23.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: dill~=0.3.3 in /usr/local/lib/python3.7/dist-packages (from medcat) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: Flask~=1.1 in /usr/local/lib/python3.7/dist-packages (from medcat) (1.1.4)\n",
            "Collecting transformers~=4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 29.9MB/s \n",
            "\u001b[?25hCollecting gensim~=3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/afe2315e08a38967f8a3036bbe7e38b428e9b7a90e823a83d0d49df1adf5/gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.7MB/s \n",
            "\u001b[?25hCollecting scipy~=1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e8/43ffca541d2f208d516296950b25fe1084b35c2881f4d444c1346ca75815/scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4MB 159kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: torch~=1.8.1 in /usr/local/lib/python3.7/dist-packages (from medcat) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch~=7.10->medcat) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch~=7.10->medcat) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas~=1.0->medcat) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas~=1.0->medcat) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets~=1.6.0->medcat) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from datasets~=1.6.0->medcat) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets~=1.6.0->medcat) (0.70.11.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 50.9MB/s \n",
            "\u001b[?25hCollecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/d2/d05466997f7751a2c06a7a416b7d1f131d765f7916698d3fdcb3a4d037e5/fsspec-2021.6.0-py3-none-any.whl (114kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 51.4MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3c/e3/fb7b6aefaf0fc7b792cebbbd590b1895c022ab0ff27f389e1019c6f2e68a/huggingface_hub-0.0.10-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets~=1.6.0->medcat) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets~=1.6.0->medcat) (4.5.0)\n",
            "Requirement already satisfied, skipping upgrade: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets~=1.6.0->medcat) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn~=0.0->medcat) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->medcat) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->medcat) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->medcat) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->medcat) (0.4.1)\n",
            "Collecting thinc<7.5.0,>=7.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/92/71ab278f865f7565c37ed6917d0f23342e4f9a0633013113bd435cf0a691/thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 35.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->medcat) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->medcat) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->medcat) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->medcat) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->medcat) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1->medcat) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1->medcat) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1->medcat) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1->medcat) (7.1.2)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 15.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers~=4.5.1->medcat) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers~=4.5.1->medcat) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 21.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim~=3.8->medcat) (5.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim~=3.8->medcat) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch~=1.8.1->medcat) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets~=1.6.0->medcat) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets~=1.6.0->medcat) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets~=1.6.0->medcat) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets~=1.6.0->medcat) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn~=0.0->medcat) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask~=1.1->medcat) (2.0.1)\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.20.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: elasticsearch, numpy, xxhash, fsspec, huggingface-hub, datasets, thinc, spacy, sacremoses, tokenizers, transformers, scipy, gensim, medcat\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed datasets-1.6.2 elasticsearch-7.13.1 fsspec-2021.6.0 gensim-3.8.3 huggingface-hub-0.0.10 medcat-1.0.33 numpy-1.20.3 sacremoses-0.0.45 scipy-1.6.3 spacy-2.3.4 thinc-7.4.5 tokenizers-0.10.3 transformers-4.5.1 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jj679NkSfsS"
      },
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFwGQA3tSzF4"
      },
      "source": [
        "DATA_DIR = \"./data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR1Hqh5wSPaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f4af142-789b-4957-8ddb-6c1bad94502d"
      },
      "source": [
        "!mkdir ./data\n",
        "!mkdir ./models\n",
        "!wget https://raw.githubusercontent.com/CogStack/MedCAT/master/tutorial/data/noteevents.csv -P ./data/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-15 00:02:04--  https://raw.githubusercontent.com/CogStack/MedCAT/master/tutorial/data/noteevents.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7171226 (6.8M) [text/plain]\n",
            "Saving to: ‘./data/noteevents.csv’\n",
            "\n",
            "noteevents.csv      100%[===================>]   6.84M  44.3MB/s    in 0.2s    \n",
            "\n",
            "2021-06-15 00:02:04 (44.3 MB/s) - ‘./data/noteevents.csv’ saved [7171226/7171226]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDBI5vy2VQRH"
      },
      "source": [
        "### Meta Annotations with MedCAT\n",
        "\n",
        "To train meta-annotations (e.g. Experiencer, Negation...) we need two additional models:\n",
        "- Tokenizer: to tokenize the text\n",
        "- Embeddings: Word2Vec or any other type of embeddings that will be used for meta annotations. \n",
        "\n",
        "For meta-annotations we will use a custom BiLSTM model with simulated attention that works very well with sub-word tokenizers and embeddings creating using Word2Vec or BERT (for simplicity we will use w2v here). All of this is also available for download (check next tutorial) and we only need to rebuild the tokenizer/embeddings if our use-case is from a very specific domain. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0ihb0tLSlRu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3b4be165-176d-4125-8b6c-d6f154d0dddf"
      },
      "source": [
        "# To train the tokenizer we will use all the data we have from our dummy dataset.\n",
        "df = pd.read_csv(DATA_DIR + \"noteevents.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>subject_id</th>\n",
              "      <th>chartdate</th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>01/01/2086</td>\n",
              "      <td>Urology</td>\n",
              "      <td>CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>01/01/2086</td>\n",
              "      <td>Emergency Room Reports</td>\n",
              "      <td>CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>01/01/2086</td>\n",
              "      <td>General Medicine</td>\n",
              "      <td>CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>01/01/2086</td>\n",
              "      <td>General Medicine</td>\n",
              "      <td>CHIEF COMPLAINT:,  Followup on hypertension an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>01/01/2086</td>\n",
              "      <td>Consult - History and Phy.</td>\n",
              "      <td>CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                               text\n",
              "0           0  ...  CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...\n",
              "1           1  ...  CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...\n",
              "2           2  ...  CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...\n",
              "3           3  ...  CHIEF COMPLAINT:,  Followup on hypertension an...\n",
              "4           4  ...  CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBtVgjCZS0gF"
      },
      "source": [
        "# The tokenizers from huggingface require us to save all the text used for \n",
        "#training into one/multiple text files.\n",
        "f = open(DATA_DIR + \"tok_data.txt\", 'w')\n",
        "for text in df['text'].values:\n",
        "  #We'll remove new lines, so that we have one document in one line\n",
        "  text = text.strip().replace(\"\\n\", ' ')\n",
        "  f.write(text.lower()) # Lowercase text to remove noise\n",
        "  f.write(\"\\n\")\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dukwUnN1TPCg"
      },
      "source": [
        "# Create, train and save the tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer.train(DATA_DIR + \"tok_data.txt\")\n",
        "tokenizer.save(\"./models/bbpe\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh9uCi6ETiH3"
      },
      "source": [
        "# Now we tokenize all the text we have and train word2vec\n",
        "f = open(DATA_DIR + \"tok_data.txt\", 'r')\n",
        "# Note that if you have a very large dataset, use iterators that\n",
        "#read the text line by line from the file, do not load the whole file\n",
        "#into memory.\n",
        "data = []\n",
        "for line in f:\n",
        "  data.append(tokenizer.encode(line).tokens)\n",
        "w2v = Word2Vec(data, size=300, min_count=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1yh0JBSUGJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518d7271-8f80-40ca-c4a5-824c5f82377c"
      },
      "source": [
        "# Check is word2vec trained, Ġ - for this tokenizer denotes start of word (a space)\n",
        "w2v.most_similar('Ġcancer')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ġcolon', 0.7464339733123779),\n",
              " ('Ġmetastatic', 0.7170583605766296),\n",
              " ('Ġbreast', 0.681978702545166),\n",
              " ('Ġmesothelioma', 0.663028359413147),\n",
              " ('Ġcarcinoma', 0.6485821008682251),\n",
              " ('Ġfather', 0.6427798867225647),\n",
              " ('Ġaugmentation', 0.6409773826599121),\n",
              " ('Ġca', 0.6338627338409424),\n",
              " ('Ġfamily', 0.619067907333374),\n",
              " ('Ġmother', 0.6181211471557617)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrRr4Pd_UgvY"
      },
      "source": [
        "# Now we just have to create the embeddings matrix\n",
        "embeddings = []\n",
        "for i in range(tokenizer.get_vocab_size()):\n",
        "  word = tokenizer.id_to_token(i)\n",
        "  if word in w2v.wv:\n",
        "    embeddings.append(w2v.wv[word])\n",
        "  else:\n",
        "    # Assign a random vector if the word was not frequent enough to receive\n",
        "    #an embedding\n",
        "    embeddings.append(np.random.rand(300))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz465LmIVD2E"
      },
      "source": [
        "# Save the embeddings\n",
        "np.save(open(\"./models/embeddings.npy\", 'wb'), np.array(embeddings))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmrmJkDq4lT7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}